import gymnasium as gym
from maze_env import MazeEnv, load_custom_points
from stable_baselines3 import TD3
from stable_baselines3.common.noise import NormalActionNoise
import numpy as np
import os
import matplotlib.pyplot as plt
from tqdm import tqdm
import time
import psutil

#please restart the training if ep_len_mean does not improve for a while ~ almost around 25,000 to 30,000 timesteps.

if __name__ == "__main__":
    start_time = time.time()
    process = psutil.Process(os.getpid())
    mem_usages = []
    cpu_usages = []
    # Load custom start/goal points if available
    start_pos, goal_pos = load_custom_points()
    env = MazeEnv(start_pos=start_pos, goal_pos=goal_pos)
    n_actions = env.action_space.shape[0]
    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))
    model = TD3(
        "MlpPolicy",
        env,
        action_noise=action_noise,
        learning_rate=2e-4,
        buffer_size=50000,
        learning_starts=1000,
        batch_size=256,
        tau=0.005,
        gamma=0.99,
        train_freq=1,
        gradient_steps=1,
        policy_delay=2,
        target_policy_noise=0.2,
        target_noise_clip=0.5,
        verbose=1,
        device='auto',
        tensorboard_log="./td3_tensorboard/",
    )
    print("\nTo visualize training metrics, run the following command in your terminal:")
    print("tensorboard --logdir ./td3_tensorboard/")
    print("Then open the provided URL in your browser to view interactive plots of ep_len_mean, ep_rew_mean, and more.")
    model.learn(total_timesteps=50000)
    # Track rewards, steps, and success per episode
    episode_rewards = []
    episode_steps = []
    success_count = 0
    num_episodes = 0

    obs, info = env.reset()
    total_reward = 0
    steps = 0

    for t in tqdm(range(50000), desc="TD3 Training Progress", ncols=80):
        action, _ = model.predict(obs, deterministic=False)
        obs, reward, done, truncated, info = env.step(action)
        total_reward += reward
        steps += 1
        if t % 1000 == 0:
            mem_usages.append(process.memory_info().rss / (1024 * 1024))  # MB
            cpu_usages.append(process.cpu_percent(interval=None))
        if done or truncated:
            episode_rewards.append(total_reward)
            episode_steps.append(steps)
            num_episodes += 1
            # Check if the agent reached the goal 
            if 'is_success' in info and info['is_success']:
                success_count += 1
            elif hasattr(env, 'agent_pos') and np.array_equal(env.agent_pos, env.goal_pos):
                success_count += 1
            obs, info = env.reset()
            total_reward = 0
            steps = 0
    end_time = time.time()
    training_time = end_time - start_time
    print(f"Total training time: {training_time:.2f} seconds")
    if mem_usages:
        print(f"Peak memory usage: {max(mem_usages):.2f} MB")
        print(f"Average CPU usage: {np.mean(cpu_usages):.2f}%")

    success_rate = (success_count / num_episodes) * 100 if num_episodes > 0 else 0
    print(f"Success Rate: {success_rate:.2f}% ({success_count}/{num_episodes})")

   
    os.makedirs("T:/PPO/DRL-based-path-finding/maze_td3/saved_models", exist_ok=True)
    model.save("T:/PPO/DRL-based-path-finding/maze_td3/saved_models/td3_maze")
    print("Model saved to T:/PPO/DRL-based-path-finding/maze_td3/saved_models/td3_maze.zip")


    # Visualize the final path generated by the trained agent
    obs, info = env.reset()
    path = [env.agent_pos.copy()]
    for _ in range(env.max_steps):
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, done, _, info = env.step(action)
        path.append(env.agent_pos.copy())
        if done:
            break
    path = np.array(path)

    # B-spline smoothing function
    def bspline_path(path, s=2):
        from scipy.interpolate import splprep, splev
        if len(path) < 4:
            return path
        tck, u = splprep([path[:,0], path[:,1]], s=s)
        unew = np.linspace(0, 1, max(100, len(path)*3))
        out = splev(unew, tck)
        return np.array(list(zip(out[0], out[1])))

    # Original path
    plt.figure(figsize=(7,7))
    plt.imshow(env.obstacles.T, origin='lower', cmap='gray_r', alpha=0.5)
    plt.plot(path[:,0], path[:,1], 'g.-', label='Agent Path')
    plt.scatter(path[0,0], path[0,1], c='blue', s=100, label='Start')
    plt.scatter(env.goal_pos[0], env.goal_pos[1], c='red', s=100, label='Goal')
    plt.title('TD3 Maze: Final Path (Original)')
    plt.legend()
    plt.grid(True)
    plt.savefig('T:/PPO/DRL-based-path-finding/maze_td3/visuals/td3_maze_final_path.png', dpi=200)
    plt.show()

   
    # B-spline smoothened path
    try:
        smoothed_bspline = bspline_path(path)
        plt.figure(figsize=(7,7))
        plt.imshow(env.obstacles.T, origin='lower', cmap='gray_r', alpha=0.5)
        plt.plot(smoothed_bspline[:,0], smoothed_bspline[:,1], 'r-', label='B-spline')
        plt.scatter(smoothed_bspline[0,0], smoothed_bspline[0,1], c='blue', s=100, label='Start')
        plt.scatter(env.goal_pos[0], env.goal_pos[1], c='red', s=100, label='Goal')
        plt.title('TD3 Maze: B-spline Path')
        plt.legend()
        plt.grid(True)
        plt.savefig('T:/PPO/DRL-based-path-finding/maze_td3/visuals/td3_maze_final_path_bspline.png', dpi=200)
        plt.show()
    except Exception as e:
        print(f'B-spline smoothing failed: {e}')
    print('Final path plots saved to T:/PPO/DRL-based-path-finding/maze_td3/visuals/td3_maze_final_path.png, td3_maze_final_path_moving_average.png, td3_maze_final_path_bspline.png')
