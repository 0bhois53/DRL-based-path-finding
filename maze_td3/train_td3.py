import gymnasium as gym
from maze_env import MazeEnv
from stable_baselines3 import TD3
from stable_baselines3.common.noise import NormalActionNoise
import numpy as np
import os
import matplotlib.pyplot as plt

if __name__ == "__main__":
    env = MazeEnv()
    n_actions = env.action_space.shape[0]
    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))
    model = TD3(
        "MlpPolicy",
        env,
        action_noise=action_noise,
        learning_rate=3e-4,
        buffer_size=50000,
        learning_starts=1000,
        batch_size=128,
        tau=0.005,
        gamma=0.99,
        train_freq=1,
        gradient_steps=1,
        policy_delay=2,
        target_policy_noise=0.2,
        target_noise_clip=0.5,
        verbose=1,
        device='auto',
    )
    model.learn(total_timesteps=50000)
    # Track rewards and steps per episode
    episode_rewards = []
    episode_steps = []
    obs, info = env.reset()
    total_reward = 0
    steps = 0
    for t in range(50000):
        action, _ = model.predict(obs, deterministic=False)
        obs, reward, done, _, info = env.step(action)
        total_reward += reward
        steps += 1
        if done:
            episode_rewards.append(total_reward)
            episode_steps.append(steps)
            obs, info = env.reset()
            total_reward = 0
            steps = 0
    os.makedirs("saved_models", exist_ok=True)
    model.save("saved_models/td3_maze")
    print("Model saved to saved_models/td3_maze.zip")
    # Plot accumulated rewards
    plt.figure()
    plt.plot(episode_rewards)
    plt.xlabel('Episode')
    plt.ylabel('Accumulated Reward')
    plt.title('TD3 Maze: Accumulated Rewards per Episode')
    plt.grid(True)
    plt.savefig('visuals/td3_maze_rewards.png', dpi=200)
    # Plot steps per episode
    plt.figure()
    plt.plot(episode_steps)
    plt.xlabel('Episode')
    plt.ylabel('Steps Taken')
    plt.title('TD3 Maze: Steps per Episode')
    plt.grid(True)
    plt.savefig('visuals/td3_maze_steps.png', dpi=200)
    print('Plots saved to visuals/td3_maze_rewards.png and visuals/td3_maze_steps.png')

    # Visualize the final path generated by the trained agent
    obs, info = env.reset()
    path = [env.agent_pos.copy()]
    for _ in range(env.max_steps):
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, done, _, info = env.step(action)
        path.append(env.agent_pos.copy())
        if done:
            break
    path = np.array(path)
    plt.figure(figsize=(7,7))
    plt.imshow(env.obstacles.T, origin='lower', cmap='gray_r', alpha=0.5)
    plt.plot(path[:,0], path[:,1], 'g.-', label='Agent Path')
    plt.scatter(path[0,0], path[0,1], c='blue', s=100, label='Start')
    plt.scatter(env.goal_pos[0], env.goal_pos[1], c='red', s=100, label='Goal')
    plt.title('TD3 Maze: Final Path')
    plt.legend()
    plt.grid(True)
    plt.savefig('visuals/td3_maze_final_path.png', dpi=200)
    plt.show()
    print('Final path plot saved to visuals/td3_maze_final_path.png')
